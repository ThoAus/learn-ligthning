{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm                           # Progress bar\n",
    "from torch.utils.data import random_split\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "\n",
    "import lightning as L                           # PyTorch Lightning\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner       # Hyperparameter tuning (example: lr_finder)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "INPUT_SIZE = 784\n",
    "NUM_CLASSES = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.1\n",
    "MC_DROPOUT_SAMPLES = 50\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Dataset\n",
    "DATA_DIR = \"dataset/\"\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Compute related\n",
    "ACCELERATOR = \"cpu\"\n",
    "DEVICES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Using the LightningDataModule\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/data/datamodule.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Dataset loaded from:\n",
    "\n",
    "https://pytorch.org/vision/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        entire_dataset = datasets.MNIST(\n",
    "            root=self.data_dir,\n",
    "            train=True,\n",
    "            transform=transforms.ToTensor(),\n",
    "            download=False,\n",
    "        )\n",
    "        self.train_ds, self.val_ds = random_split(entire_dataset, [50000, 10000])\n",
    "        self.test_ds = datasets.MNIST(\n",
    "            root=self.data_dir,\n",
    "            train=False,\n",
    "            transform=transforms.ToTensor(),\n",
    "            download=False,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            persistent_workers=True,        # Considered to speed up the dataloader worker initialization\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            persistent_workers=True,        # Considered to speed up the dataloader worker initialization\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            persistent_workers=True,        # Considered to speed up the dataloader worker initialization\n",
    "        )\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_ds,                    # Use the entire validation dataset for prediction\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            persistent_workers=True,        # Considered to speed up the dataloader worker initialization\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Using the LightningModule\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/common/lightning_module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_2_layer(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 20)\n",
    "        self.fc2 = nn.Linear(20, self.num_classes)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN_2_layer(100, 3, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(range(100))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.5243,  7.5379, -4.3157], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModule(L.LightningModule):\n",
    "    def __init__(self, input_size, learning_rate, num_classes, dropout_rate, dropout_samples):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_samples = dropout_samples\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(input_size, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, self.num_classes)\n",
    "\n",
    "        # Add a dropout layer between all the layers\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # Loss function and metrics\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # Common step for training, validation, and test steps, to avoid code duplication\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        return loss, scores, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        accuracy = self.accuracy(scores, y)\n",
    "        self.log_dict({\"train_loss\": loss,\"train_acc\": accuracy},\n",
    "                      on_step=False,\n",
    "                      on_epoch=True,\n",
    "                      prog_bar=True,\n",
    "                      )\n",
    "        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        accuracy = self.accuracy(scores, y)\n",
    "        self.log_dict({\"val_loss\": loss,\"val_acc\": accuracy},\n",
    "                      on_step=False,\n",
    "                      on_epoch=True,\n",
    "                      prog_bar=True,\n",
    "                      )\n",
    "        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    # Prediction step applying Monte Carlo Dropout\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        # Enable dropout\n",
    "        self.dropout.train()\n",
    "        predictions = torch.zeros(self.dropout_samples, x.size(0), self.num_classes)\n",
    "        for i in range(self.dropout_samples):\n",
    "            scores = self.forward(x)\n",
    "            predictions[i] = F.softmax(scores, dim=1)\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Using the Trainer\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/common/trainer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningModule(input_size=INPUT_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE,\n",
    "            dropout_samples=MC_DROPOUT_SAMPLES,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dropout_rate\":    0.1\n",
       "\"dropout_samples\": 50\n",
       "\"input_size\":      784\n",
       "\"learning_rate\":   0.001\n",
       "\"num_classes\":     10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MnistDataModule(data_dir=DATA_DIR,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     num_workers=NUM_WORKERS,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = CSVLogger(\"logs\", name=\"FCM_MNIST\")\n",
    "\n",
    "trainer = L.Trainer(# accelerator=ACCELERATOR,\n",
    "                    # devices=DEVICES,\n",
    "                    max_epochs=NUM_EPOCHS,\n",
    "                    logger=logger,\n",
    "                    # profiler=\"simple\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuner - find hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the learning rate finder to find the optimal learning rate\n",
    "lr_finder = tuner.lr_find(model, train_dataloaders=dm)\n",
    "\n",
    "# Plot the learning rate finder results\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum batch size that fits in memory\n",
    "# In this case not necessary, because the dataset is small\n",
    "# tuner.scale_batch_size(model, train_dataloaders=dm, steps_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | fc1      | Linear             | 15.7 K\n",
      "1 | fc2      | Linear             | 420   \n",
      "2 | fc3      | Linear             | 210   \n",
      "3 | dropout  | Dropout            | 0     \n",
      "4 | loss_fn  | CrossEntropyLoss   | 0     \n",
      "5 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "16.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.3 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85b9eb4c2d5419fba364eba2daa7d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba66b2f768ed4ef5b697fdccf51968b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b74d7341d54472913d724f3c46b596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb92c1d2114e3885899c77cdd8aa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b71fb380bd44bd688c54771db462c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30a2ba28f0b4ffab93932943e1ed3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db279fb69b24bac9b9d09c6c39f9524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d949e8d7210d44ba890518a1011f3ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f54c3cf6f04b4d941e85ec047e707e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a5165a1fb74fd29e9aaae791606e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebafe2570164b76a42bbedbffe7684d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525c5e27da62452482995a18892ee5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logged metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv file in a pandas dataframe to visualize the metrics\n",
    "# Select the version folder\n",
    "version = 0\n",
    "df = pd.read_csv(f\"logs/FCM_MNIST/version_{version}/metrics.csv\")\n",
    "\n",
    "# Drop the NaN values, summarize the metrics for each epoch\n",
    "df = df.groupby(\"epoch\").median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "fig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "# Plot the loss\n",
    "ax[0].plot(df.index, df[\"train_loss\"], label=\"training\", color=\"r\")\n",
    "ax[0].plot(df.index, df[\"val_loss\"], label=\"validation\", color=\"b\")\n",
    "\n",
    "# Plot the accuracy\n",
    "ax[1].plot(df.index, df[\"train_acc\"], label=\"training\", color=\"r\")\n",
    "ax[1].plot(df.index, df[\"val_acc\"], label=\"validation\", color=\"b\")\n",
    "# Set the y-axis limits\n",
    "ax[1].set_ylim(0, 1)\n",
    "\n",
    "# Set the labels\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the import and plot of the metrics in a single function\n",
    "def plot_metrics(version):\n",
    "    df = pd.read_csv(f\"logs/FCM_MNIST/version_{version}/metrics.csv\")\n",
    "    df = df.groupby(\"epoch\").median()\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "    ax[0].plot(df.index, df[\"train_loss\"], label=\"training\", color=\"r\")\n",
    "    ax[0].plot(df.index, df[\"val_loss\"], label=\"validation\", color=\"b\")\n",
    "\n",
    "    ax[1].plot(df.index, df[\"train_acc\"], label=\"training\", color=\"r\")\n",
    "    ax[1].plot(df.index, df[\"val_acc\"], label=\"validation\", color=\"b\")\n",
    "    ax[1].set_ylim(0, 1)\n",
    "\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions and calculate uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor from the prediction list\n",
    "predictions = pred[0]\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean softmax probabilities\n",
    "# Take the class with the highest probability\n",
    "idx_max = predictions.mean(dim=0).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean softmax probabilities\n",
    "# Take the column of std according to the idx_max\n",
    "softmax_max = predictions.mean(dim=0)[torch.arange(predictions.size(1)), idx_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of the softmax probabilities\n",
    "# Take the column of std according to the idx_max\n",
    "std_max = predictions.std(dim=0)[torch.arange(predictions.size(1)), idx_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the results\n",
    "df = pd.DataFrame({\"Prediction\": idx_max.numpy(),\n",
    "                   \"Target\":dm.test_ds.targets[0:BATCH_SIZE].numpy(),\n",
    "                   \"Softmax_UC\": 1 - softmax_max.numpy(),\n",
    "                   \"Dropout_UC\": std_max.numpy(),\n",
    "                   })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Softmax_UC against the Dropout_UC\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df[\"Softmax_UC\"],\n",
    "            df[\"Dropout_UC\"],\n",
    "            c=df[\"Prediction\"] == df[\"Target\"],\n",
    "            cmap=\"bwr\",\n",
    "            alpha=0.5,\n",
    "            s=50)\n",
    "\n",
    "plt.xlabel(\"Softmax Uncertainty\")\n",
    "plt.ylabel(\"Dropout Uncertainty\")\n",
    "plt.title(f\"Dropout Samples = {MC_DROPOUT_SAMPLES} \\n Dropout Rate = {DROPOUT_RATE}\")\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots/Samples_{MC_DROPOUT_SAMPLES}_Rate_{DROPOUT_RATE}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the Softmax_UC and the Dropout_UC\n",
    "df[[\"Softmax_UC\", \"Dropout_UC\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
